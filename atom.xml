<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  
    <title type="text" xml:lang="en">Jordan Buysse</title>
    <link type="application/atom+xml" rel="self" href="http://jdbuysse.github.io//atom.xml"/>
  
  <link href="http://jdbuysse.github.io//"/>
  <id>http://jdbuysse.github.io//</id>
  <updated>2020-08-07T20:15:26Z</updated>
  <author>
    <name>Jordan Buysse</name>
    <email>jdbuysse@gmail.com</email>
  </author>
  <rights type="text">Copyright © 2020 Jordan Buysse. All rights reserved.</rights>
  
  <entry>
  <title type="text">Metaprogramming and Ruby</title>
  <link rel="alternate" type="text/html" href="http://jdbuysse.github.io//metaprogramming-ruby.html" />
  <id>http://jdbuysse.github.io//metaprogramming-ruby</id>
  <published>2020-05-28T00:00:00Z</published>
  <updated>2020-05-28T00:00:00Z</updated>
  <content type="html"><![CDATA[ <p>At Flatiron, we start out with Ruby. I can’t say I love the language on an aesthetic level (implicit returns and the ‘do’ keyword grate on me for some reason), but it was worth this brief foray into Ruby just to see get some exposure to Ruby’s take on metaprogramming. On a conceptual level, metaprogramming refers to processes where languages can read, generate, or modify pieces of themselves. I’m not going to delve into the theoretical stuff here, though. In practice I’ve found this often means blurring the distinction between compilation and runtime, a practice that in its most radical form via Lisp–as Paul Graham says “the whole language is always available”–including all the intermediary bits of in-between computer speak.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup> What this all means for Ruby, in my experience so far, is two things.</p>

<h2 id="monkey-patching">Monkey-patching</h2>

<p>The first is ‘monkey-patching’–the ability to amend methods in Ruby’s default classes on the fly. For example, maybe you’re on island time. You could dig in to Ruby’s default classes and change the definition of the weekend:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>module CoreExtensions
  	 module DateTime
	  module BusinessDays
  	   def weekday?
        !sunday? &amp;&amp; !saturday? &amp;&amp; !friday?
  	   end
	 end
  	end
end
</code></pre></div></div>

<p><a href="https://www.justinweiss.com/articles/3-ways-to-monkey-patch-without-making-a-mess/">Thanks to Justin Weiss</a> for the example. There’s not much benefit to doing this via monkey patching, of course, and it’s certainly not how I’d solve the problem in the wild. But this example goes to show how you can build robust packages for Ruby based on modifying core ‘upstream’ classes in the Ruby default library. “Awesome Print” was a popular one for our Mod 1 projects, which runs via modifications of Ruby’s default IRB (aka “interactive Ruby” aka REPL aka Ruby’s read-eval-print loop). “Awesome Print” brings Ruby’s “Pretty Print” to life via full colors and new features. But as the docs say, Awesome Print really is just “exposing their internal structure with proper indentation,” building alongside rather than on top of Ruby’s existing methods.</p>
<h2 id="metaprogramming-to-define-new-classes-and-methods">Metaprogramming to define new classes and methods</h2>
<p>The other example I want to talk about very briefly is ActiveRecord, a source of a lot of the ‘magic’ people talk about when they talk about Rails and which, I’d argue, is the key piece of tech that allows the Flatiron curriculum to jump from CLIs to non-templated full-stack web apps so quickly. <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">2</a></sup> There’s way to much to cover on the topic of AR and metaprogramming so I’ll just note one simple example: Active Record’s “Dynamic Finders,” which make ugly database queries into friendly (yes, ‘magic’, feeling) natural language queries. If you have a field for spirit_animal on your User model for example, AR will generate a method called find_by_spirit_animal for free. find_by_ functions are simple plug-and-play templates that insert the field name into a method with everything else defined.
Metaprogramming! It’s everywhere. Especially in the ‘magic’ parts of Rails magic. I wrote this blog mostly to try and take a nebulous, theoretical concept, and pin it down to a couple of things I’ve done in Ruby. Hope you can find some use for it.</p>

<h2 id="endnotes">Endnotes</h2>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>As a side note, Lisp was the first programming language I ever had someone actually try to teach me, circa 2003, when I was in middle school. I was at a computer camp of some sort—maybe the actual curriculum was learning to build websites in Dreamweaver, or make flash animations, or something? All I really remember was two things. The first was really only wanting to play Age of Empires, which was installed on all the computers. The second was that one of the instructors there taught a mini class on Lisp programming. I remember reading the Lisp docs for some reason, not understanding a thing, especially a cryptic mention of illegal lambda functions, whatever that meant. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>From what I understand, other campuses take a lengthier route through Sinatra other Rack related things first <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
 ]]></content>
</entry>


  <entry>
  <title type="text">Reading Grammar: First Attempts</title>
  <link rel="alternate" type="text/html" href="http://jdbuysse.github.io//reading-grammar.html" />
  <id>http://jdbuysse.github.io//reading-grammar</id>
  <published>2019-10-14T00:00:00Z</published>
  <updated>2019-10-14T00:00:00Z</updated>
  <content type="html"><![CDATA[ <blockquote>
  <p><em>Cross-posted to the Scholars’ Lab Blog at https://scholarslab.lib.virginia.edu/blog/reading-grammar/</em></p>
</blockquote>

<p>I am not a grammar whiz. I couldn’t tell you off the top of my head what makes a verb transitive, much less how to identify a “nominal subject.” I haven’t taken a linguistics class or really thought all that much about the field past Saussure (the starting point for a lot of 101-level literary theory classes) and Chomsky (the point where mainline literary theory seems to diverge from linguistics). On the spectrum between descriptivism and prescriptivism I’m an apathetic descriptivist. But in writing the last couple chapters of my dissertation, I suddenly found myself brushing up on the basics. I’m taking something that I’ve known by feel for 20 years and trying to understand a bit more about how it’s formalized. One of the tools I’ve been using is not really from linguistics but rather out-of-date writing pedagogy.</p>

<p>In <a href="https://scholarslab.lib.virginia.edu/blog/introducing-gs-grammars/">my first post</a> I gave some backstory for my fellowship project studying grammar structures in Gertrude Stein. I’m going to use this post to show how I’ve implemented one piece of the puzzle. What I think is really interesting about this example, from a digital humanities perspective, is the way that computation can resurrect some old forms or inquiry that people have neither the time nor inclination for. I’d struggle mightily to diagram a single sentence by hand, but I’ve generated hundreds of diagrams while writing about Stein. They have changed how I read them. With all the big claims about what machine learning does to our experience of the world here’s a very minor, very specific one: reconstructing an old pedagogical task in order to inhabit an old way of thinking about composition.</p>

<p>Like I mentioned above, I haven’t spent a lot of time thinking about how grammar structures are formalized. This is probably in large part because I’ve been word-processing digitally more or less since I started writing. I’ve always had those squiggly underlines to tell me when I had something wrong with my grammar. From the late 1800s to the 1960s or so when the practice began to fall out of fashion, American schools had students drill “sentence diagrams” in order to learn writing and the basic tenets of grammar.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup></p>

<h2 id="reviving-the-grammar-tree">Reviving the Grammar Tree</h2>

<p>So what do sentence diagrams actually look like? There’s no one standard, even though an 1877 textbook by Alonzo Reed and Brainerd Kellogg was the most influential in writing pedagogy of the late 19th century. A lot of the hand-drawn diagrams look something like this:</p>

<p><img src="/static/post-media/2019-10-14-reading-grammar/RK_diagram.gif" alt="An image from the 1877 Reed Kellogg textbook *English for Use*" /></p>

<p>It’s safe to assume Stein would have been creating diagrams that looked similar. I’ve been generating diagrams using the natural language processing package SpaCy for Python and a module called <a href="https://spacy.io/universe/project/explacy">explacy</a>. Explacy ‘prints’ out grammar trees in a monospaced font as a low-tech way of generating grammar diagrams of various length.</p>

<p><img src="/static/post-media/2019-10-14-reading-grammar/rose.png" alt="A tree diagram with spaCy and explacy" /></p>

<p>For a while I was using these tools locally, but eventually it seemed worth finding a way to put them online. You can try out a (very rough!) demo of the sentence parser ‘<a href="http://grammar.click/">here</a>’.</p>

<p><img src="/static/post-media/2019-10-14-reading-grammar/diag1.png" alt="My online demo of spaCy + explacy." /></p>

<p>I started thinking about sentence diagrams because they appear to have been quite formative to the young Stein. If other students dreaded the rote (and as it turns out fairly useless) “grammar school” task, Stein loved the systematic approach to language:</p>

<blockquote>
  <p>When you are at school and learn grammar grammar is very exciting. 
I really do not know that anything has ever been more exciting than
diagramming sentences. I suppose other things may be more exciting 
to others when they are at school but to me undoubtedly when I was at 
school the really completely exciting thing was diagramming sentences 
and that has been to me ever since the one thing that has been 
completely exciting and completely completing. I like the feeling the 
everlasting feeling of sentences as they diagram themselves. 
(<em>Lectures in America</em>  210-211)</p>
</blockquote>

<p>When I use modern natural language processing technologies to generate sentence diagrams, I get to share in Stein’s act of “completely completing” an analysis the skeleton of a sentence. Neither do I feel like I’m missing too much by automating the process–even Stein, who was drawing the diagrams by hand, phrases the activity as something inherent in the language–sentences “diagram themselves.”</p>

<p>Participating in the act of diagramming gives me not only a glimpse of how Stein and her American contemporaries would have been trained to think about composition, but also a better technical vocabulary for closely reading the mechanics of Steinese. William H. Gass once wrote about seven pages closely reading Stein’s use of the word ‘the’ in a single sentence from Stein’s <em>Three Lives</em>. It’s even an interesting read! Gass’ eye for the subtleties of ‘the’ is a skill that only comes with a super-analytic approach towards language mechanics, drilled from childhood. I frankly don’t have the stamina or skill set for that sort of reading, but by crunching the numbers with a computer rather than working by hand, I can at least borrow the correct terminology. For as often as digital humanities gets branded as being about new methods, sometimes it’s worth simply resurrecting the old. You learn something about the linguistic world your subjects lived in, and pick up some maneuvers of your own along the way.</p>

<h2 id="endnotes">Endnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Grammar was always important to pedagogy, a component of the lower “trivium” within the classical hierarchy of the liberal arts, alongside logic and rhetoric. But Americans, apparently, <a href="https://www.npr.org/sections/ed/2014/08/22/341898975/a-picture-of-language-the-fading-art-of-diagramming-sentences">took the idea of studying grammar quite literally.</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
 ]]></content>
</entry>


  <entry>
  <title type="text">Introducing 'Gertrude Stein's Grammars'</title>
  <link rel="alternate" type="text/html" href="http://jdbuysse.github.io//introducing-gs-grammars.html" />
  <id>http://jdbuysse.github.io//introducing-gs-grammars</id>
  <published>2019-09-11T00:00:00Z</published>
  <updated>2019-09-11T00:00:00Z</updated>
  <content type="html"><![CDATA[ <blockquote>
  <p><em>Cross-posted to the Scholars’ Lab Blog at https://scholarslab.lib.virginia.edu/blog/introducing-gs-grammars/</em></p>
</blockquote>

<p>Hi! I’m Jordan Buysse, a sixth year PhD candidate in the English department and one of the Scholars’ Lab fellows this year. I work on modern and contemporary literature, media studies, and text analysis. My dissertation is about the relationship between literary works and information. In math and computer science, information refers to a type of communication that has nothing to do with meaning. But this definition is at odds with the colloquial sense of the word and its longer etymological history, which is about in<em>form</em>ing people–shaping the mind. As it turns out, these two opposite senses of the term actually have a lot to say about what goes on when we read different types of literary works.
So far I’ve written chapters about novels that work like search engines and a poet who writes all of her work algorithmically. For my fellowship project and the associated chapter, I’m working on the modernist poet Gertrude Stein. My work with Stein is a little bit different because, while the other chapters have been heavily informed by ‘data’-centric approaches to literary studies, it is the first chapter where I am doing significant computational work for the sake of my argument. I’m going to get into how that works in future blog posts, but here I actually want to look back at how my thinking has developed.
My work with Stein starts a long time ago in my first year of grad school. I was really interested in some of the natural language processing tools that were being developed at the time, in particular things like the <a href="https://nlp.stanford.edu/software/lex-parser.shtml">Stanford parser</a> that could determine and tag the parts of speech for a given text.</p>

<p><img src="static/img/2019-09-11-stanford-parser-diagrams.png" alt="Two diagrams I generated with the Stanford Parser in 2015" /></p>

<p>I presented an early incarnation of this research at the 2015 ACLA conference under the title “Tender Buttons and Regular Expressions.” The earlier paper on Tender Buttons mostly revolved around Stein sentences that were actually questions although they contained no question mark. I used this specific case to demonstrate that, while these questions were invisible to “bag of words” tokenization, they were quite easy to track using grammar parsers and regular expressions. Grammar parsing also proved to eliminate a large number of false positives, flagging a sentence like “what is the wind, what is it” while passing over sentences like “A large box is handily made of what is necessary to replace any substance.” The increased technical difficulty of using grammar parsers over n-grams or regular expressions is justified by these edge cases, which are abundant in “Steinese” 
My methods back then were fairly laborious–I used the Stanford Parser with different pre-trained models to tag the initial text, and then adapted individual Python scripts to spit out ‘cleaned’ versions of the text according to different features. Below you can see screenshots of .txt files with the initial markup and with extracted matching features.</p>

<p><img src="static/img/2019-09-11-stein-parsed-tags.png" alt="The raw grammar-tagged text from the Stanford parser" /></p>

<p><img src="static/img/2019-09-11-stein-parse-clean.png" alt="Output from a Python script I wrote to isolate features from the text and ‘clean’ them by removing tags" /></p>

<p>For this initial paper, there were some fancier theoretical arguments about digital versus analog ‘reading,’ but the truth is that I was much more interested in seeing if the methods worked than making any specific literary-critical argument about Stein.
I left the Stein project alone for a while because I wasn’t sure what to do with it. While I certainly saw a lot of potential in the methods I used here, the ability to isolate questions itself wasn’t going to do much for a larger argument about her work. As time went on, there were some remarkable advances in the tools available for grammar parsing that led me to revisit this initial work in the area. I’ll talk more about those in blog posts later this semester. In a lot of ways, what I’m working on now is a more elegant solution to questions I was posing back in 2015. Instead of tagging text files one by one and writing custom regular expressions to reassemble the text in different forms, my current work streamlines the process to make it accessible to anyone with an interest in posing questions about the shape of grammar to a given text. I’ll leave it at that for now, but watch this space for more in the coming months!</p>

 ]]></content>
</entry>



</feed>
